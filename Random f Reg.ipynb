{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df=pd.read_csv('..\\Data Files\\CSV files\\weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()\n",
    "# We need to drop all rows with missing values on rain today, rain tomorrow as this can affect our model. Reason is that one is our target variable and the other is closely related to it.\n",
    "weather_df.dropna(subset= ['RainTomorrow'],inplace=True)\n",
    "\n",
    "#Declaring inputs and outputs\n",
    "input_cols=(weather_df.columns)[1:-1]\n",
    "target_col='RainTomorrow'\n",
    "\n",
    "#Declaring numerical cols and categorical cols\n",
    "numeric_cols=weather_df[input_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols=weather_df[input_cols].select_dtypes('object').columns.tolist()\n",
    "\n",
    "# Check for missing values and Perform imputation\n",
    "weather_df[numeric_cols].isna().sum()\n",
    "\n",
    "# Check for missing values and Perform imputation\n",
    "weather_df[numeric_cols].isna().sum()\n",
    "#Imputing our data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy='mean')\n",
    "imputer.fit(weather_df[numeric_cols])\n",
    "# Divide/Split the data to train,validation and test df frames\n",
    "year=pd.to_datetime(weather_df.Date).dt.year\n",
    "\n",
    "train_df=weather_df[year<2015]\n",
    "val_df=weather_df[year==2015]\n",
    "test_df=weather_df[year>2015]\n",
    "\n",
    "# Divide/Split the data to train,validation and test df frames\n",
    "year=pd.to_datetime(weather_df.Date).dt.year\n",
    "\n",
    "train_df=weather_df[year<2015]\n",
    "val_df=weather_df[year==2015]\n",
    "test_df=weather_df[year>2015]\n",
    "\n",
    "train_inputs=train_df[input_cols].copy()\n",
    "train_targets=train_df[target_col].copy\n",
    "val_inputs=val_df[input_cols].copy()\n",
    "val_targets=val_df[target_col].copy()\n",
    "test_inputs=test_df[input_cols].copy()\n",
    "test_targets=test_df[target_col].copy()\n",
    "#To fill the columns with imputed statistics..\n",
    "train_inputs[numeric_cols]=imputer.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols]=imputer.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols]=imputer.transform(test_inputs[numeric_cols])\n",
    "#To view imputer statistics\n",
    "list(imputer.statistics_)\n",
    "\n",
    "# Check for missing values\n",
    "val_inputs[numeric_cols].isna().sum()\n",
    "#Scaling Numerical Values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler().fit(weather_df[numeric_cols])\n",
    "train_inputs[numeric_cols]=scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols]=scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols]=scaler.transform(test_inputs[numeric_cols])\n",
    "#Dealing with missing values\n",
    "train_inputs[categorical_cols]=train_inputs[categorical_cols].fillna('Unknown')\n",
    "val_inputs[categorical_cols]=val_inputs[categorical_cols].fillna('Unknown')\n",
    "test_inputs[categorical_cols]=test_inputs[categorical_cols].fillna('Unknown')\n",
    "#check out the no of unique values in the categorical columns \n",
    "weather_df[categorical_cols].nunique()\n",
    "\n",
    "#check out the no of unique values in the categorical columns \n",
    "weather_df[categorical_cols].nunique()\n",
    "#Encoding Categorical Columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
    "encoder.fit(weather_df[categorical_cols])\n",
    "\n",
    "#We generate columns for each individual category using using get_feature_names\n",
    "encoded_cols=list(encoder.get_feature_names(categorical_cols))\n",
    "print(encoded_cols)\n",
    "#We generate columns for each individual category using using get_feature_names\n",
    "encoded_cols=list(encoder.get_feature_names(categorical_cols))\n",
    "print(encoded_cols)\n",
    "train_inputs[encoded_cols]=encoder.transform(train_inputs[categorical_cols])\n",
    "val_inputs[encoded_cols]=encoder.transform(val_inputs[categorical_cols])\n",
    "test_inputs[encoded_cols]=encoder.transform(test_inputs[categorical_cols])\n",
    "#Dropping the textualised columns so that we are left with just numeric data\n",
    "x_train=train_inputs[numeric_cols+encoded_cols]\n",
    "x_val=val_inputs[numeric_cols+encoded_cols]\n",
    "x_test=test_inputs[numeric_cols+encoded_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs allows the random forest to use mutiple parallel workers to train decision trees, and random_state=42 ensures that the we get the same results for each execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, train_targets)\n",
    "\n",
    "model.score(x_train, train_targets)\n",
    "\n",
    "model.score(x_val, val_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the training accuracy is almost 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests?\n",
    "\n",
    "This general technique of combining the results of many models is called \"ensembling\", it works because most errors of individual models cancel out on averaging. Here's what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities for the predictions. \n",
    "train_probs = model.predict_proba(x_train)\n",
    "train_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access individual decision trees using\n",
    "model.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model.estimators_[0], max_depth=2, feature_names=x_train.columns, filled=True, rounded=True, class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning with Random forests\n",
    "#We first create a basem odel in which we can compare models with tuned hyperparameters\n",
    "base_model=RandomForestClassifier(random_state=42,n_jobs=1).fit(x_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_acc=base_model.score(x_train,train_targets)\n",
    "base_val_acc=base_model.score(x_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_acc=base_train_acc,base_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters\n",
    "   n_estimators=controls the number of decision trees in the random forest. Default is 100. for larger data it helps to have a greater number of estimators\n",
    "\n",
    "ma_depth and max_leaf_nodes= pass directly to the decison treee. by default no maax_depth is specified which is why each tree has a training accuracy of 100%. Specifying max_depth reduces overfitting.\n",
    "\n",
    "max_features={'auto','sqrt','log2'}, int or float default='auto.instead of picking alll features(columns) for every split, we can specify only a fraction of features to be randomly chosen to figure out a split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4122881f0b18059c084b807e98d84da441e13384cb191732a25f1fd7b110f4b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
